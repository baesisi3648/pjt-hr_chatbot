"""
ZIC-TALK HR ì±—ë´‡ - LangGraph ì›Œí¬í”Œë¡œìš° ì—”ì§„
ëŒ€í™” ë§¥ë½ì„ ì´í•´í•˜ê³  3ì¤‘ ê²€ì¦(Draft-Critic-Rewrite)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""
import os
from typing import TypedDict, Literal, List, Dict
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_core.messages import SystemMessage, HumanMessage
from langgraph.graph import StateGraph, END

# í™˜ê²½ ì„¤ì •
load_dotenv()

# ========== ì„¤ì • ìƒìˆ˜ ==========
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME", "company-rules")
PINECONE_NAMESPACE = "rules-2025"
RETRIEVER_K = int(os.getenv("RETRIEVER_K", "5"))
MAX_CHAT_HISTORY = int(os.getenv("MAX_CHAT_HISTORY", "6"))
MAX_REVISION_COUNT = int(os.getenv("MAX_REVISION_COUNT", "2"))

# ========== ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ==========
REWRITE_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ëŒ€í™” ë§¥ë½ì„ ì´í•´í•˜ì—¬ ì§ˆë¬¸ì„ ì¬ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ í˜„ì¬ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™”ë¥¼ ì°¸ì¡°í•˜ëŠ” ê²½ìš°(ì˜ˆ: "ê·¸ëŸ¼ ê·¸ê±´?", "ë” ì•Œë ¤ì¤˜"), 
ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **ë…ë¦½ì ì´ê³  ëª…í™•í•œ ì§ˆë¬¸**ìœ¼ë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”.

ì˜ˆì‹œ:
- ì´ì „ ëŒ€í™”ì—ì„œ "ì—°ì°¨"ì— ëŒ€í•´ ì´ì•¼ê¸°í–ˆê³ , í˜„ì¬ ì§ˆë¬¸ì´ "ê·¸ëŸ¼ ì›”ì°¨ëŠ”?"ì´ë©´
  â†’ "ì·¨ì—…ê·œì¹™ì—ì„œ ì›”ì°¨ íœ´ê°€ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
  
- ì´ì „ ëŒ€í™” ì—†ì´ "ì—°ì°¨ëŠ” ëª‡ì¼ì¸ê°€ìš”?"ë¼ê³  ë¬¼ìœ¼ë©´
  â†’ "ì·¨ì—…ê·œì¹™ì—ì„œ ì—°ì°¨ íœ´ê°€ëŠ” ëª‡ì¼ì¸ê°€ìš”?" (ê·¸ëŒ€ë¡œ ìœ ì§€ ë˜ëŠ” ëª…í™•í™”)

**ì¤‘ìš”**: 
- ì¬ì‘ì„±ëœ ì§ˆë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ë¶€ê°€ ë¬¸êµ¬ ì—†ì´.
- ì·¨ì—…ê·œì¹™/ì¸ì‚¬ê·œì • ë§¥ë½ì„ ìœ ì§€í•˜ì„¸ìš”."""

DRAFT_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ íšŒì‚¬ ì·¨ì—…ê·œì¹™ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ ê·œì • ì›ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

**ë‹µë³€ ì›ì¹™**:
1. ê·œì •ì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ë‹µë³€ (ì¶”ì¸¡ ê¸ˆì§€)
2. ì¡°í•­ ë²ˆí˜¸ì™€ í•¨ê»˜ ê·¼ê±°ë¥¼ ëª…í™•íˆ ì œì‹œ
3. ì‚¬ìš©ì ì¹œí™”ì ì¸ ì„¤ëª… ì¶”ê°€
4. ê·œì •ì— ì—†ëŠ” ë‚´ìš©ì´ë©´ "í•´ë‹¹ ë‚´ìš©ì€ ê·œì •ì— ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€"""

CRITIQUE_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ì—„ê²©í•œ ì‚¬ì‹¤ ê²€ì¦ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ ë‹µë³€ì´ ê·œì • ì›ë¬¸ì— **ì •í™•íˆ ì¼ì¹˜**í•˜ëŠ”ì§€ ê²€ì¦í•˜ì„¸ìš”.

**í‰ê°€ ê¸°ì¤€**:
- PASS: ëª¨ë“  ë‚´ìš©ì´ ê·œì •ì— ê·¼ê±°í•˜ë©° ì‚¬ì‹¤ê³¼ ì¼ì¹˜
- FAIL: ê·œì •ì— ì—†ëŠ” ë‚´ìš© ì¶”ì¸¡, ì˜ëª»ëœ í•´ì„, ì¡°í•­ ë²ˆí˜¸ ì˜¤ë¥˜ ë“±

**ì¶œë ¥ í˜•ì‹**:
í‰ê°€: PASS ë˜ëŠ” FAIL
ì´ìœ : (FAILì¸ ê²½ìš° êµ¬ì²´ì ì¸ ë¬¸ì œì  ì§€ì )"""

# ========== ìƒíƒœ ì •ì˜ ==========
class GraphState(TypedDict):
    question: str                       # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ì§ˆë¬¸ (ë³€í™˜ëœ ì¿¼ë¦¬)
    original_question: str              # ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸
    context: str                        # ê²€ìƒ‰ëœ ê·œì • ì›ë¬¸
    draft: str                          # ìƒì„±ëœ ë‹µë³€ ì´ˆì•ˆ
    critique: str                       # ê°ì‚¬ê´€ì˜ ì§€ì ì‚¬í•­
    grade: str                          # í‰ê°€ ê²°ê³¼ (PASS / FAIL)
    revision_count: int                 # ìˆ˜ì • íšŸìˆ˜
    chat_history: List[Dict[str, str]]  # ëŒ€í™” ê¸°ë¡

# ========== ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ==========
embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)
vector_store = PineconeVectorStore.from_existing_index(
    index_name=PINECONE_INDEX_NAME,
    embedding=embeddings,
    namespace=PINECONE_NAMESPACE
)
retriever = vector_store.as_retriever(search_kwargs={"k": RETRIEVER_K})
llm = ChatOpenAI(model=OPENAI_MODEL, temperature=0)

# ========== ë…¸ë“œ í•¨ìˆ˜ë“¤ ==========
def rewrite_question(state: GraphState) -> GraphState:
    """ëŒ€í™” ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬ í˜„ì¬ ì§ˆë¬¸ì„ ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ì¬ì‘ì„±"""
    question = state["original_question"]
    chat_history = state.get("chat_history", [])
    
    if chat_history and len(chat_history) > 0:
        # ìµœê·¼ ëŒ€í™”ë§Œ ì°¸ê³ 
        recent_history = chat_history[-MAX_CHAT_HISTORY:]
        history_text = "\n".join([
            f"{'ì‚¬ìš©ì' if msg['role'] == 'user' else 'AI'}: {msg['content']}"
            for msg in recent_history
        ])
        
        messages = [
            SystemMessage(content=REWRITE_SYSTEM_PROMPT),
            HumanMessage(content=f"""ì´ì „ ëŒ€í™”:
{history_text}

í˜„ì¬ ì§ˆë¬¸: {question}

ì¬ì‘ì„±ëœ ì§ˆë¬¸:""")
        ]
        
        response = llm.invoke(messages)
        rewritten = response.content.strip()
        
        print(f"\nğŸ”„ [ì§ˆë¬¸ ì¬ì‘ì„±]")
        print(f"   ì›ë³¸: {question}")
        print(f"   ì¬ì‘ì„±: {rewritten}")
        
        state["question"] = rewritten
    else:
        state["question"] = question
        print(f"\nğŸ“ [ì²« ì§ˆë¬¸] {question}")
    
    return state


def retrieve_context(state: GraphState) -> GraphState:
    """ë²¡í„° DBì—ì„œ ê´€ë ¨ ê·œì •ì„ ê²€ìƒ‰"""
    question = state["question"]
    print(f"\nğŸ” [ê·œì • ê²€ìƒ‰] '{question}'ì— ëŒ€í•œ ê´€ë ¨ ì¡°í•­ ê²€ìƒ‰ ì¤‘...")
    
    docs = retriever.invoke(question)
    
    context_parts = []
    for i, doc in enumerate(docs, 1):
        article_title = doc.metadata.get("article_title", "Unknown")
        content = doc.page_content
        context_parts.append(f"[ë¬¸ì„œ {i}] {article_title}\n{content}")
    
    context = "\n\n---\n\n".join(context_parts)
    state["context"] = context
    
    print(f"   âœ… ì´ {len(docs)}ê°œì˜ ê´€ë ¨ ì¡°í•­ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    return state


def generate_draft(state: GraphState) -> GraphState:
    """ê²€ìƒ‰ëœ ê·œì •ì„ ë°”íƒ•ìœ¼ë¡œ ì´ˆì•ˆì„ ì‘ì„±"""
    question = state["question"]
    context = state["context"]
    chat_history = state.get("chat_history", [])
    
    print(f"\nâœï¸  [ì´ˆì•ˆ ì‘ì„±] ë‹µë³€ ìƒì„± ì¤‘...")
    
    # ëŒ€í™” ê¸°ë¡ì„ ê°„ë‹¨íˆ ìš”ì•½í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
    history_context = ""
    if chat_history and len(chat_history) > 0:
        recent = chat_history[-4:]
        history_context = "\n\nì´ì „ ëŒ€í™” ì°¸ê³ :\n" + "\n".join([
            f"- {msg['role']}: {msg['content'][:100]}..."
            for msg in recent
        ])
    
    messages = [
        SystemMessage(content=f"""{DRAFT_SYSTEM_PROMPT}

**ê²€ìƒ‰ëœ ê·œì •**:
{context}
{history_context}
"""),
        HumanMessage(content=question)
    ]
    
    response = llm.invoke(messages)
    draft = response.content
    state["draft"] = draft
    
    print(f"   âœ… ì´ˆì•ˆ ì‘ì„± ì™„ë£Œ (ê¸¸ì´: {len(draft)} ê¸€ì)")
    return state


def critique_answer(state: GraphState) -> GraphState:
    """ì‘ì„±ëœ ë‹µë³€ì„ íŒ©íŠ¸ì²´í¬í•˜ê³  í‰ê°€"""
    draft = state["draft"]
    context = state["context"]
    question = state["question"]
    
    print(f"\nğŸ” [íŒ©íŠ¸ì²´í¬] ë‹µë³€ ê²€ì¦ ì¤‘...")
    
    messages = [
        SystemMessage(content=CRITIQUE_SYSTEM_PROMPT),
        HumanMessage(content=f"""ì§ˆë¬¸: {question}

ë‹µë³€:
{draft}

ê·œì • ì›ë¬¸:
{context}

í‰ê°€ë¥¼ ì‹œì‘í•˜ì„¸ìš”:""")
    ]
    
    response = llm.invoke(messages)
    critique = response.content
    state["critique"] = critique
    
    if "PASS" in critique.split('\n')[0].upper():
        state["grade"] = "PASS"
        print(f"   âœ… ê²€ì¦ í†µê³¼!")
    else:
        state["grade"] = "FAIL"
        print(f"   âŒ ê²€ì¦ ì‹¤íŒ¨ - ìˆ˜ì • í•„ìš”")
        print(f"   ì‚¬ìœ : {critique[:100]}...")
    
    return state


def rewrite_answer(state: GraphState) -> GraphState:
    """í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ë‹µë³€ì„ ìˆ˜ì •"""
    draft = state["draft"]
    critique = state["critique"]
    context = state["context"]
    question = state["question"]
    
    state["revision_count"] = state.get("revision_count", 0) + 1
    
    print(f"\nğŸ”§ [ë‹µë³€ ìˆ˜ì •] {state['revision_count']}ì°¨ ìˆ˜ì • ì¤‘...")
    
    messages = [
        SystemMessage(content=f"""ë‹¹ì‹ ì€ í”¼ë“œë°±ì„ ë°›ì•„ ë‹µë³€ì„ ê°œì„ í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ê²€ì¦ í”¼ë“œë°±**:
{critique}

**ê·œì • ì›ë¬¸**:
{context}

ìœ„ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ë‹µë³€ì„ ìˆ˜ì •í•˜ì„¸ìš”. ë°˜ë“œì‹œ ê·œì •ì— ê·¼ê±°í•œ ë‚´ìš©ë§Œ í¬í•¨í•˜ì„¸ìš”.
"""),
        HumanMessage(content=f"""ì§ˆë¬¸: {question}

ê¸°ì¡´ ë‹µë³€:
{draft}

ìˆ˜ì •ëœ ë‹µë³€:""")
    ]
    
    response = llm.invoke(messages)
    revised = response.content
    state["draft"] = revised
    
    print(f"   âœ… ìˆ˜ì • ì™„ë£Œ")
    return state


def should_continue(state: GraphState) -> Literal["rewrite", "end"]:
    """ë‹µë³€ì´ í†µê³¼í–ˆëŠ”ì§€, ì¬ì‘ì„±ì´ í•„ìš”í•œì§€ íŒë‹¨"""
    if state["grade"] == "PASS":
        return "end"
    
    if state.get("revision_count", 0) >= MAX_REVISION_COUNT:
        print("\nâš ï¸  ìµœëŒ€ ìˆ˜ì • íšŸìˆ˜ ë„ë‹¬ - í˜„ì¬ ë‹µë³€ìœ¼ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return "end"
    
    return "rewrite"


# ========== ê·¸ë˜í”„ êµ¬ì„± ==========
workflow = StateGraph(GraphState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("rewrite_question", rewrite_question)
workflow.add_node("retrieve", retrieve_context)
workflow.add_node("generate", generate_draft)
workflow.add_node("critique", critique_answer)
workflow.add_node("rewrite", rewrite_answer)

# ì—£ì§€ ì—°ê²°
workflow.set_entry_point("rewrite_question")
workflow.add_edge("rewrite_question", "retrieve")
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", "critique")

# ì¡°ê±´ë¶€ ì—£ì§€
workflow.add_conditional_edges(
    "critique",
    should_continue,
    {
        "rewrite": "rewrite",
        "end": END
    }
)
workflow.add_edge("rewrite", "critique")

# ì»´íŒŒì¼
app = workflow.compile()


# ========== ì‹¤í–‰ í—¬í¼ í•¨ìˆ˜ ==========
def run_workflow(question: str, chat_history: List[Dict[str, str]] = None):
    """
    ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•˜ê³  ìµœì¢… ë‹µë³€ì„ ë°˜í™˜
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        chat_history: ì´ì „ ëŒ€í™” ê¸°ë¡ [{"role": "user", "content": "..."}, ...]
    
    Returns:
        ìµœì¢… ë‹µë³€ ë¬¸ìì—´
    """
    if chat_history is None:
        chat_history = []
    
    inputs = {
        "original_question": question,
        "question": question,
        "context": "",
        "draft": "",
        "critique": "",
        "grade": "",
        "revision_count": 0,
        "chat_history": chat_history
    }
    
    result = app.invoke(inputs)
    return result["draft"]


# ========== í…ŒìŠ¤íŠ¸ ì½”ë“œ ==========
if __name__ == "__main__":
    print("="*80)
    print("ZIC-TALK ì±—ë´‡ í…ŒìŠ¤íŠ¸")
    print("="*80)
    
    # ì²« ë²ˆì§¸ ì§ˆë¬¸
    history = []
    q1 = "ì—°ì°¨ëŠ” ì–¼ë§ˆë‚˜ ì£¼ë‚˜ìš”?"
    print(f"\nğŸ‘¤ ì‚¬ìš©ì: {q1}")
    answer1 = run_workflow(q1, history)
    print(f"\nğŸ¤– AI: {answer1}")
    
    # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
    history.append({"role": "user", "content": q1})
    history.append({"role": "assistant", "content": answer1})
    
    # í›„ì† ì§ˆë¬¸ (ëŒ€í™” ë§¥ë½ ì°¸ì¡°)
    q2 = "ê·¸ëŸ¼ ì›”ì°¨ëŠ”?"
    print(f"\nğŸ‘¤ ì‚¬ìš©ì: {q2}")
    answer2 = run_workflow(q2, history)
    print(f"\nğŸ¤– AI: {answer2}")
