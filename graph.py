import os
import json
from typing import TypedDict, Literal
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_core.messages import SystemMessage, HumanMessage
from langgraph.graph import StateGraph, END

# 1. í™˜ê²½ ì„¤ì •
load_dotenv()

# 2. ìƒíƒœ(State) ì •ì˜
class GraphState(TypedDict):
    question: str           # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ì§ˆë¬¸ (ë³€í™˜ëœ ì¿¼ë¦¬)
    original_question: str  # ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸ (ì°¸ê³ ìš©)
    context: str            # ê²€ìƒ‰ëœ ê·œì • ì›ë¬¸
    draft: str              # ìƒì„±ëœ ë‹µë³€ ì´ˆì•ˆ
    critique: str           # ê°ì‚¬ê´€ì˜ ì§€ì ì‚¬í•­
    grade: str              # í‰ê°€ ê²°ê³¼ (PASS / FAIL)
    revision_count: int     # ìˆ˜ì • íšŸìˆ˜ (ë¬´í•œ ë£¨í”„ ë°©ì§€)

# 3. ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vector_store = PineconeVectorStore.from_existing_index(
    index_name=os.environ.get("PINECONE_INDEX_NAME", "company-rules"),
    embedding=embeddings,
    namespace="rules-2025"
)
retriever = vector_store.as_retriever(search_kwargs={"k": 5})

# ëª¨ë¸ ì„¤ì •
# llm_draft: ë³µì¡í•œ ì¶”ë¡ ì´ í•„ìš”í•œ ì´ˆì•ˆ ì‘ì„±ìš© (GPT-4o)
# llm_critic & transformer: ë‹¨ìˆœ ì‘ì—… ë° ê²€ì¦ìš© (GPT-4o-mini) - ì†ë„/ë¹„ìš© ìµœì í™”
llm_draft = ChatOpenAI(model="gpt-4o", temperature=0)
llm_critic = ChatOpenAI(model="gpt-4o-mini", temperature=0, model_kwargs={"response_format": {"type": "json_object"}})
llm_transformer = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# ==========================================
# 4. ë…¸ë“œ(Node) í•¨ìˆ˜ ì •ì˜
# ==========================================

def transform_query_node(state: GraphState):
    """
    [0ë‹¨ê³„] ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬, ê²€ìƒ‰ í™•ë¥ ì„ ë†’ì´ëŠ” 'ìµœì ì˜ ê²€ìƒ‰ì–´'ë¡œ í™•ì¥/ë³€í™˜í•©ë‹ˆë‹¤.
    (í•˜ë“œì½”ë”©ëœ ë‹¨ì–´ì¥ ì—†ì´ LLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í™œìš©í•©ë‹ˆë‹¤.)
    """
    print("\nğŸ”„ [0] ì§ˆë¬¸ í™•ì¥(Query Expansion) ì¤‘...")
    question = state["question"]
    
    # ë³€í™˜ìš© LLM (gpt-4o-mini)
    llm_transformer = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    
    prompt = f"""
    ë‹¹ì‹ ì€ ê¸°ì—… ì¸ì‚¬(HR) ê·œì • ê²€ìƒ‰ì„ ìœ„í•œ 'ê²€ìƒ‰ì–´ ìµœì í™” ì „ë¬¸ê°€'ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ êµ¬ì–´ì²´ë‚˜ ë¹„ê³µì‹ ìš©ì–´(ì€ì–´)ê°€ ì„ì—¬ ìˆì–´, ê·œì •ì§‘ ê²€ìƒ‰(Vector DB) ì‹œ ì •í™•ë„ê°€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    [ë‹¹ì‹ ì˜ ì„ë¬´]
    1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ íŒŒì•…í•˜ì„¸ìš”.
    2. ì§ˆë¬¸ì— í¬í•¨ëœ í•µì‹¬ ë‹¨ì–´ë¥¼ **'ê¸°ì—… ì·¨ì—…ê·œì¹™'ì—ì„œ ì£¼ë¡œ ì“°ì´ëŠ” ê³µì‹ ë²•ë¥ /í–‰ì • ìš©ì–´**ë¡œ ë³€í™˜í•˜ì„¸ìš”.
    3. í˜¹ì‹œ ëª¨ë¥¼ ìƒí™©ì— ëŒ€ë¹„í•´ **ìœ ì˜ì–´(Synonyms)**ë„ í•¨ê»˜ í¬í•¨í•˜ì—¬ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ í’ì„±í•˜ê²Œ ë§Œë“œì„¸ìš”.
    4. ê²°ê³¼ëŠ” ì˜¤ì§ **ë³€í™˜ëœ ê²€ìƒ‰ì–´ ë¬¸ì¥**ë§Œ ì¶œë ¥í•˜ì„¸ìš”. (ì„¤ëª… ê¸ˆì§€)

    [ì˜ˆì‹œ]
    User: "íšŒì‚¬ ë©°ì¹  ì•ˆ ë‚˜ì˜¤ë©´ ì˜ë ¤?"
    AI: "ë¬´ë‹¨ê²°ê·¼ ì‹œ ì§ê¶Œë©´ì§ ê¸°ì¤€ ë° ì§•ê³„ í•´ê³  ì‚¬ìœ  (ê²°ê·¼, ë¬´ê³„ê²°ê·¼)"
    
    User: "ì•  ë‚³ìœ¼ë©´ ì–¸ì œê¹Œì§€ ì‰¬ì–´?"
    AI: "ì¶œì‚°ì „í›„íœ´ê°€ ê¸°ê°„ ë° ìœ¡ì•„íœ´ì§ ì‹ ì²­ ê°€ëŠ¥ ê¸°ê°„ (ëª¨ì„±ë³´í˜¸)"

    [ì‚¬ìš©ì ì§ˆë¬¸]
    {question}
    """
    
    # LLMì´ ìŠ¤ìŠ¤ë¡œ ìƒê°í•´ì„œ ê²€ìƒ‰ì–´ë¥¼ ë§Œë“­ë‹ˆë‹¤.
    better_question = llm_transformer.invoke([HumanMessage(content=prompt)]).content
    print(f"   ğŸ‘‰ í™•ì¥ëœ ì¿¼ë¦¬: '{better_question}'")
    
    return {"question": better_question, "original_question": question}

def retrieve_node(state: GraphState):
    """[1ë‹¨ê³„] ë³€í™˜ëœ ì§ˆë¬¸ìœ¼ë¡œ ê·œì •ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    print("\nğŸ” [1] ê²€ìƒ‰ ì¤‘...")
    question = state["question"]
    docs = retriever.invoke(question)
    context = "\n\n---\n\n".join([doc.page_content for doc in docs])
    return {"context": context, "revision_count": 0}

def draft_node(state: GraphState):
    print("\nğŸ“ [2] ì´ˆì•ˆ ì‘ì„± ì¤‘...")
    question = state["question"]
    context = state["context"]

    # í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ + ìœ ì—°í•œ í•´ì„ì„ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompt = """
    ë‹¹ì‹ ì€ ì¸ì²œë©”íŠ¸ë¡œì„œë¹„ìŠ¤ ê·œì •ì§‘ ê¸°ë°˜ì˜ íŒ©íŠ¸ì²´í¬ ë´‡ì…ë‹ˆë‹¤. 
    ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì œê³µëœ [ì°¸ê³ í•  ì·¨ì—…ê·œì¹™]ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

    [ë‹µë³€ ì‘ì„± ì›ì¹™ (ì¤‘ìš”)]
    1. **[ìœ ì˜ì–´ í•´ì„ í—ˆìš©]:** ì‚¬ìš©ìëŠ” 'ë¬´ë‹¨ê²°ê·¼', 'ì§¤ë¦°ë‹¤', 'ì›”ê¸‰' ê°™ì€ ì¼ìƒ ìš©ì–´ë¥¼ ì“°ì§€ë§Œ, ê·œì •ì§‘ì€ 'ë¬´ê³„ê²°ê·¼', 'ì§ê¶Œë©´ì§', 'ë³´ìˆ˜' ê°™ì€ í–‰ì • ìš©ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
       - ì§ˆë¬¸ì˜ ë‹¨ì–´ê°€ ê·œì •ì˜ ë‹¨ì–´ì™€ 100% ì¼ì¹˜í•˜ì§€ ì•Šë”ë¼ë„, **ì˜ë¯¸ê°€ ë™ì¼í•˜ë‹¤ë©´ ê´€ë ¨ ê·œì •ìœ¼ë¡œ íŒë‹¨í•˜ê³  ë‹µë³€í•˜ì„¸ìš”.**
       - ì˜ˆ: ì§ˆë¬¸ "ë¬´ë‹¨ê²°ê·¼" -> ê·œì • "ë¬´ê³„ê²°ê·¼" (ë‹µë³€ ê°€ëŠ¥ O)
    
    2. **[í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€]:** ìœ„ ìœ ì˜ì–´ í•´ì„ì„ ì ìš©í–ˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³ , ì „í˜€ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©(ì˜ˆ: ì¬íƒê·¼ë¬´)ì´ë¼ë©´ "ê·œì •ì— ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ê³  ì¢…ë£Œí•˜ì„¸ìš”.
    
    3. **[ë‹µë³€ ìŠ¤íƒ€ì¼]:**
       - í•µì‹¬ ê²°ë¡ ì„ ë¨¼ì € ë§í•˜ê³ , ë¬¸ì¥ ëì— **ê·¼ê±° ì¡°í•­(ì˜ˆ: ì œ12ì¡° ì œ4í•­)**ì„ ê´„í˜¸ë¡œ ëª…ì‹œí•˜ì„¸ìš”.
       - "ì¼ë°˜ì ìœ¼ë¡œ", "í†µìƒì ìœ¼ë¡œ" ê°™ì€ ì‚¬ì¡±ì€ ë¶™ì´ì§€ ë§ˆì„¸ìš”.
    """
    
    user_message = f"""
    [ì°¸ê³ í•  ì·¨ì—…ê·œì¹™]
    {context}

    [ì§ˆë¬¸]
    {question}
    """
    
    messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]
    response = llm_draft.invoke(messages)
    return {"draft": response.content}

def critic_node(state: GraphState):
    """[3ë‹¨ê³„] ë‹µë³€ì„ ê²€ì¦í•©ë‹ˆë‹¤ (JSON ì¶œë ¥)."""
    print("\nğŸ•µï¸ [3] íŒ©íŠ¸ì²´í¬ ì¤‘...")
    context = state["context"]
    draft = state["draft"]

    prompt = f"""
    ë‹¹ì‹ ì€ ì—„ê²©í•œ ê·œì • ì¤€ìˆ˜ ê°ì‚¬ê´€ì…ë‹ˆë‹¤. ì´ˆì•ˆì´ ë‹¤ìŒ ê¸°ì¤€ì„ ìœ„ë°˜í–ˆëŠ”ì§€ ê²€ì‚¬í•˜ì„¸ìš”.
    
    [ê²€ì¦ ê¸°ì¤€]
    1. [ê·œì • ì›ë¬¸]ì— ì—†ëŠ” ë‚´ìš©(ì™¸ë¶€ ì§€ì‹, ì¼ë°˜ ìƒì‹)ì´ í¬í•¨ë˜ì—ˆëŠ”ê°€? -> í¬í•¨ë˜ë©´ **FAIL**
    2. "ì¼ë°˜ì ìœ¼ë¡œ", "í†µìƒì ìœ¼ë¡œ", "ê¶Œì¥í•©ë‹ˆë‹¤" ê°™ì€ **ë‡Œí”¼ì…œ ì¡°ì–¸**ì´ í¬í•¨ë˜ì—ˆëŠ”ê°€? -> í¬í•¨ë˜ë©´ **FAIL**
    3. ê·œì •ì— ì—†ëŠ” ì§ˆë¬¸ì— ëŒ€í•´ "ê·œì •ì— ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ê¹”ë”í•˜ê²Œ ê±°ì ˆí–ˆëŠ”ê°€? -> ê±°ì ˆ í›„ ì‚¬ì¡±ì„ ë¶™ì˜€ë‹¤ë©´ **FAIL**
    4. ê·¼ê±° ì¡°í•­(ì œOì¡°)ì´ ëª…ì‹œë˜ì—ˆëŠ”ê°€? (ê·œì •ì— ìˆëŠ” ê²½ìš°)
    
    [ê·œì • ì›ë¬¸]
    {context}
    
    [ì´ˆì•ˆ ë‹µë³€]
    {draft}

    [ì¶œë ¥ í˜•ì‹ - JSON]
    {{
        "grade": "PASS" ë˜ëŠ” "FAIL",
        "critique": "PASSë©´ 'ì í•©', FAILì´ë©´ êµ¬ì²´ì ì¸ ì§€ì  ì‚¬í•­"
    }}
    """
    response = llm_critic.invoke([HumanMessage(content=prompt)])
    result = json.loads(response.content)
    
    return {"grade": result["grade"], "critique": result["critique"]}

def rewrite_node(state: GraphState):
    """[4ë‹¨ê³„] ì§€ì ì‚¬í•­ì„ ë°˜ì˜í•˜ì—¬ ë‹µë³€ì„ ìˆ˜ì •í•©ë‹ˆë‹¤."""
    print("\nâœï¸ [4] ë‹µë³€ ìˆ˜ì • ì¤‘...")
    draft = state["draft"]
    critique = state["critique"]
    revision_count = state["revision_count"]

    prompt = f"""
    ë‹¹ì‹ ì€ í¸ì§‘ìì…ë‹ˆë‹¤. ê°ì‚¬ê´€ì˜ ì§€ì ì„ ë°˜ì˜í•˜ì—¬ ë‹µë³€ì„ ìˆ˜ì •í•˜ì„¸ìš”.
    
    [ê¸°ì¡´ ì´ˆì•ˆ]
    {draft}
    
    [ì§€ì  ì‚¬í•­]
    {critique}
    
    ìœ„ ë‚´ìš©ì„ ë°˜ì˜í•˜ì—¬ ë” ì™„ë²½í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”. (ì™¸ë¶€ ì§€ì‹ ê¸ˆì§€ ì›ì¹™ ì¤€ìˆ˜)
    """
    response = llm_draft.invoke([HumanMessage(content=prompt)])
    
    return {"draft": response.content, "revision_count": revision_count + 1}

# ==========================================
# 5. ê·¸ë˜í”„(Workflow) ì—°ê²°
# ==========================================

def check_pass_or_fail(state: GraphState):
    """ì¡°ê±´ë¶€ ì—£ì§€: ê²€ì¦ ê²°ê³¼ì— ë”°ë¼ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •"""
    grade = state["grade"]
    count = state["revision_count"]

    if grade == "PASS":
        print("   âœ… ê²€ì¦ í†µê³¼!")
        return "pass"
    elif count >= 3:
        print("   ğŸ›‘ ìˆ˜ì • íšŸìˆ˜ ì´ˆê³¼ (ê·¸ëƒ¥ ë°˜í™˜)")
        return "max_retries"
    else:
        print(f"   âŒ ê²€ì¦ ì‹¤íŒ¨ (ì´ìœ : {state['critique']}) -> ì¬ì‘ì„±")
        return "rewrite"

# ê·¸ë˜í”„ ìƒì„±
workflow = StateGraph(GraphState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("transform_query", transform_query_node)
workflow.add_node("retrieve", retrieve_node)
workflow.add_node("draft", draft_node)
workflow.add_node("critic", critic_node)
workflow.add_node("rewrite", rewrite_node)

# ì—£ì§€ ì—°ê²°
workflow.set_entry_point("transform_query") # ì‹œì‘ì : ì§ˆë¬¸ ë³€í™˜
workflow.add_edge("transform_query", "retrieve")
workflow.add_edge("retrieve", "draft")
workflow.add_edge("draft", "critic")

# ì¡°ê±´ë¶€ ë¶„ê¸°
workflow.add_conditional_edges(
    "critic",
    check_pass_or_fail,
    {
        "pass": END,
        "max_retries": END,
        "rewrite": "rewrite"
    }
)

# ë£¨í”„ ì—°ê²°
workflow.add_edge("rewrite", "critic")

# ì»´íŒŒì¼
app = workflow.compile()

# ==========================================
# 6. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì½”ë“œ
# ==========================================
if __name__ == "__main__":
    print("ğŸ¤– HR ì±—ë´‡ ì—”ì§„ ì‹œë™ (Query Rewriting í¬í•¨)...")
    
    # ì€ì–´ê°€ í¬í•¨ëœ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    test_query = "íšŒì‚¬ ë©°ì¹  ì•ˆê°€ë©´ ì§¤ë ¤?"
    
    inputs = {"question": test_query}
    final_state = app.invoke(inputs)
    
    print("\nFINAL ANSWER:")
    print(final_state["draft"])